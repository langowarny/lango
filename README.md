# Lango ðŸš€

A high-performance AI agent built with Go, supporting multiple AI providers, channels (Telegram, Discord, Slack), and a self-learning knowledge system.

## Features

- ðŸ”¥ **Fast** - Single binary, <100ms startup, <100MB memory
- ðŸ¤– **Multi-Provider AI** - OpenAI, Anthropic, Gemini, Ollama with unified interface
- ðŸ”Œ **Multi-Channel** - Telegram, Discord, Slack support
- ðŸ› ï¸ **Rich Tools** - Shell execution, file system operations, browser automation, crypto & secrets tools
- ðŸ§  **Self-Learning** - Knowledge store, learning engine, skill system, observational memory
- ðŸ“Š **Knowledge Graph & Graph RAG** - BoltDB triple store with hybrid vector + graph retrieval
- ðŸ”€ **Multi-Agent Orchestration** - Hierarchical sub-agents (executor, researcher, planner, memory-manager)
- ðŸŒ **A2A Protocol** - Agent-to-Agent protocol for remote agent discovery and integration
- ðŸ”’ **Secure** - AES-256-GCM encryption, key registry, secret management, output scanning
- ðŸ’¾ **Persistent** - Ent ORM with SQLite session storage
- ðŸŒ **Gateway** - WebSocket/HTTP server for control plane
- ðŸ”‘ **Auth** - OIDC authentication, OAuth login flow

## Quick Start

### Installation

```bash
# Build from source
git clone https://github.com/langowarny/lango.git
cd lango
make build

# Or install directly
go install github.com/langowarny/lango/cmd/lango@latest
```

### Configuration

All configuration is stored in an encrypted SQLite database (`~/.lango/lango.db`), protected by a passphrase (AES-256-GCM). No plaintext config files are stored on disk.

Use the interactive onboard wizard for first-time setup:

```bash
lango onboard
```

### Run

```bash
lango serve

# Validate configuration
lango config validate
```

The onboard wizard guides you through:
1. AI provider configuration (API keys, models)
2. Server and channel setup (Telegram, Discord, Slack)
3. Security settings (encryption, signer mode, approval workflows)
4. Tool configuration
5. Knowledge and observational memory settings
6. Embedding & RAG configuration (provider, model, RAG toggle)
7. Graph Store configuration (backend, database path, traversal depth)
8. Multi-Agent mode (single vs hierarchical orchestration)
9. A2A Protocol settings (agent card, remote agents)

### CLI Commands

```
lango serve                      Start the gateway server
lango version                    Print version and build info
lango onboard                    Interactive TUI configuration wizard
lango doctor [--fix] [--json]    Diagnostics and health checks

lango config list                List all configuration profiles
lango config create <name>       Create a new profile with defaults
lango config use <name>          Switch to a different profile
lango config delete <name>       Delete a profile (--force to skip prompt)
lango config import <file>       Import and encrypt a JSON config (--profile <name>, source file is deleted after import)
lango config export <name>       Export active profile as JSON (requires passphrase)
lango config validate            Validate the active profile

lango security status [--json]   Show security configuration status
lango security migrate-passphrase Rotate encryption passphrase
lango security secrets list      List stored secrets (values hidden)
lango security secrets set <n>   Store an encrypted secret
lango security secrets delete <n> Delete a stored secret (--force)

lango memory list [--json]       List observational memory entries
lango memory status [--json]     Show memory system status
lango memory clear [--force]     Clear all memory entries

lango graph status [--json]      Show graph store status
lango graph query [flags] [--json] Query graph triples (--subject, --predicate, --object, --limit)
lango graph stats [--json]       Show graph statistics
lango graph clear [--force]      Clear all graph data

lango agent status [--json]      Show agent mode and configuration
lango agent list [--json] [--check] List local and remote agents
```

### Diagnostics

Run the doctor command to check your setup:

```bash
# Check configuration and environment
lango doctor

# Auto-fix common issues
lango doctor --fix

# JSON output for scripting
lango doctor --json
```

## Architecture

```
lango/
â”œâ”€â”€ cmd/lango/              # CLI entry point (cobra)
â”œâ”€â”€ internal/
â”‚   â”œâ”€â”€ adk/                # Google ADK agent wrapper, session/state adapters
â”‚   â”œâ”€â”€ agent/              # Agent types, PII redactor, secret scanner
â”‚   â”œâ”€â”€ app/                # Application bootstrap, wiring, tool registration
â”‚   â”œâ”€â”€ approval/           # Composite approval provider for sensitive tools
â”‚   â”œâ”€â”€ bootstrap/          # Application bootstrap: DB, crypto, config profile init
â”‚   â”œâ”€â”€ channels/           # Telegram, Discord, Slack integrations
â”‚   â”œâ”€â”€ cli/                # CLI commands
â”‚   â”‚   â”œâ”€â”€ agent/          #   lango agent status/list
â”‚   â”‚   â”œâ”€â”€ common/         #   shared CLI helpers
â”‚   â”‚   â”œâ”€â”€ doctor/         #   lango doctor (diagnostics)
â”‚   â”‚   â”œâ”€â”€ graph/          #   lango graph status/query/stats/clear
â”‚   â”‚   â”œâ”€â”€ memory/         #   lango memory list/status/clear
â”‚   â”‚   â”œâ”€â”€ onboard/        #   lango onboard (TUI wizard)
â”‚   â”‚   â”œâ”€â”€ prompt/         #   interactive prompt utilities
â”‚   â”‚   â”œâ”€â”€ security/       #   lango security status/secrets/migrate-passphrase
â”‚   â”‚   â””â”€â”€ tui/            #   TUI components and views
â”‚   â”œâ”€â”€ config/             # Config loading, env var substitution, validation
â”‚   â”œâ”€â”€ configstore/        # Encrypted config profile storage (Ent-backed)
â”‚   â”œâ”€â”€ a2a/                # A2A protocol server and remote agent loading
â”‚   â”œâ”€â”€ embedding/          # Embedding providers (OpenAI, Google, local) and RAG
â”‚   â”œâ”€â”€ ent/                # Ent ORM schemas and generated code
â”‚   â”œâ”€â”€ gateway/            # WebSocket/HTTP server, OIDC auth
â”‚   â”œâ”€â”€ graph/              # BoltDB triple store, Graph RAG, entity extractor
â”‚   â”œâ”€â”€ knowledge/          # Knowledge store, 8-layer context retriever
â”‚   â”œâ”€â”€ learning/           # Learning engine, error pattern analyzer, self-learning graph
â”‚   â”œâ”€â”€ logging/            # Zap structured logger
â”‚   â”œâ”€â”€ memory/             # Observational memory (observer, reflector, token counter)
â”‚   â”œâ”€â”€ orchestration/      # Multi-agent orchestration (executor, researcher, planner, memory-manager)
â”‚   â”œâ”€â”€ passphrase/         # Passphrase prompt and validation helpers
â”‚   â”œâ”€â”€ provider/           # AI provider interface and implementations
â”‚   â”‚   â”œâ”€â”€ anthropic/      #   Claude models
â”‚   â”‚   â”œâ”€â”€ gemini/         #   Google Gemini models
â”‚   â”‚   â””â”€â”€ openai/         #   OpenAI-compatible (GPT, Ollama, etc.)
â”‚   â”œâ”€â”€ security/           # Crypto providers, key registry, secrets store, companion discovery
â”‚   â”œâ”€â”€ session/            # Ent-based SQLite session store
â”‚   â”œâ”€â”€ skill/              # Skill registry, executor, builder
â”‚   â”œâ”€â”€ supervisor/         # Provider proxy, privileged tool execution
â”‚   â””â”€â”€ tools/              # browser, crypto, exec, filesystem, secrets
â”œâ”€â”€ prompts/                # Default prompt .md files (embedded via go:embed)
â””â”€â”€ openspec/               # Specifications (OpenSpec workflow)
```

## AI Providers

Lango supports multiple AI providers with a unified interface. Provider aliases are resolved automatically (e.g., `gpt`/`chatgpt` -> `openai`, `claude` -> `anthropic`, `llama` -> `ollama`, `bard` -> `gemini`).

### Supported Providers
- **OpenAI** (`openai`): GPT-4o, GPT-4, and OpenAI-compatible APIs
- **Anthropic** (`anthropic`): Claude Sonnet 4, Claude 3.5, Claude 3
- **Gemini** (`gemini`): Google Gemini models
- **Ollama** (`ollama`): Local models via Ollama (default: `http://localhost:11434/v1`)

### Setup

Use `lango onboard` to interactively configure providers, models, security settings, embedding & RAG, knowledge, and observational memory. The TUI allows you to manage multiple providers and set up local encryption.

## Configuration Reference

All settings are managed via `lango onboard` or `lango config` and stored encrypted in the profile database.

| Key | Type | Default | Description |
|-----|------|---------|-------------|
| **Server** | | | |
| `server.host` | string | `localhost` | Bind address |
| `server.port` | int | `18789` | Listen port |
| `server.httpEnabled` | bool | `true` | Enable HTTP API endpoints |
| `server.wsEnabled` | bool | `true` | Enable WebSocket server |
| `server.allowedOrigins` | []string | `[]` | WebSocket CORS allowed origins (empty = same-origin, `["*"]` = allow all) |
| **Agent** | | | |
| `agent.provider` | string | `anthropic` | Primary AI provider ID |
| `agent.model` | string | - | Primary model ID |
| `agent.fallbackProvider` | string | - | Fallback provider ID |
| `agent.fallbackModel` | string | - | Fallback model ID |
| `agent.maxTokens` | int | `4096` | Max tokens |
| `agent.temperature` | float | `0.7` | Generation temperature |
| `agent.systemPromptPath` | string | - | Legacy: single file to override the Identity section only |
| `agent.promptsDir` | string | - | Directory of `.md` files to override default prompt sections (takes precedence over `systemPromptPath`) |
| **Providers** | | | |
| `providers.<id>.type` | string | - | Provider type (openai, anthropic, gemini) |
| `providers.<id>.apiKey` | string | - | Provider API key |
| `providers.<id>.baseUrl` | string | - | Custom base URL (e.g. for Ollama) |
| **Logging** | | | |
| `logging.level` | string | `info` | Log level |
| `logging.format` | string | `console` | `json` or `console` |
| **Session** | | | |
| `session.databasePath` | string | `~/.lango/data.db` | SQLite path |
| `session.ttl` | duration | - | Session TTL before expiration |
| `session.maxHistoryTurns` | int | - | Maximum history turns per session |
| **Security** | | | |
| `security.signer.provider` | string | `local` | `local`, `rpc`, or `enclave` |
| `security.interceptor.enabled` | bool | `true` | Enable AI Privacy Interceptor |
| `security.interceptor.redactPii` | bool | `false` | Redact PII from AI interactions |
| `security.interceptor.approvalRequired` | bool | `false` | (deprecated) Require approval for sensitive tool use |
| `security.interceptor.approvalPolicy` | string | `dangerous` | Approval policy: `dangerous`, `all`, `configured`, `none` |
| `security.interceptor.approvalTimeoutSec` | int | `30` | Seconds to wait for approval before timeout |
| `security.interceptor.notifyChannel` | string | - | Channel for approval notifications (`telegram`, `discord`, `slack`) |
| `security.interceptor.sensitiveTools` | []string | - | Tool names that require approval (e.g. `["exec", "browser"]`) |
| `security.interceptor.exemptTools` | []string | - | Tool names exempt from approval regardless of policy |
| `security.interceptor.piiRegexPatterns` | []string | - | Custom regex patterns for PII detection |
| **Auth** | | | |
| `auth.providers.<id>.issuerUrl` | string | - | OIDC issuer URL |
| `auth.providers.<id>.clientId` | string | - | OIDC client ID |
| `auth.providers.<id>.clientSecret` | string | - | OIDC client secret |
| `auth.providers.<id>.redirectUrl` | string | - | OAuth callback URL |
| `auth.providers.<id>.scopes` | []string | - | OIDC scopes (e.g. `["openid", "email"]`) |
| **Tools** | | | |
| `tools.exec.defaultTimeout` | duration | - | Default timeout for shell commands |
| `tools.exec.allowBackground` | bool | `true` | Allow background processes |
| `tools.exec.workDir` | string | - | Working directory (empty = current) |
| `tools.filesystem.maxReadSize` | int | - | Maximum file size to read |
| `tools.filesystem.allowedPaths` | []string | - | Allowed paths (empty = allow all) |
| `tools.browser.enabled` | bool | `false` | Enable browser automation tools (requires Chromium) |
| `tools.browser.headless` | bool | `true` | Run browser in headless mode |
| `tools.browser.sessionTimeout` | duration | `5m` | Browser session timeout |
| **Knowledge** | | | |
| `knowledge.enabled` | bool | `false` | Enable self-learning knowledge system |
| `knowledge.maxLearnings` | int | - | Max learning entries per session |
| `knowledge.maxKnowledge` | int | - | Max knowledge entries per session |
| `knowledge.maxContextPerLayer` | int | - | Max context items per layer in retrieval |
| `knowledge.autoApproveSkills` | bool | `false` | Auto-approve new skills |
| `knowledge.maxSkillsPerDay` | int | - | Rate limit for skill creation |
| **Observational Memory** | | | |
| `observationalMemory.enabled` | bool | `false` | Enable observational memory system |
| `observationalMemory.provider` | string | - | LLM provider for observer/reflector (empty = agent default) |
| `observationalMemory.model` | string | - | Model for observer/reflector (empty = agent default) |
| `observationalMemory.messageTokenThreshold` | int | `1000` | Token threshold to trigger observation |
| `observationalMemory.observationTokenThreshold` | int | `2000` | Token threshold to trigger reflection |
| `observationalMemory.maxMessageTokenBudget` | int | `8000` | Max token budget for recent messages in context |
| **Embedding** | | | |
| `embedding.providerID` | string | - | Provider ID from `providers` map (e.g., `"gemini-1"`, `"my-openai"`). Backend type and API key are auto-resolved. |
| `embedding.provider` | string | - | Embedding backend (`openai`, `google`, `local`). Deprecated when `providerID` is set. |
| `embedding.model` | string | - | Embedding model identifier |
| `embedding.dimensions` | int | - | Embedding vector dimensionality |
| `embedding.local.baseUrl` | string | `http://localhost:11434/v1` | Local (Ollama) embedding endpoint |
| `embedding.local.model` | string | - | Model override for local provider |
| `embedding.rag.enabled` | bool | `false` | Enable RAG context injection |
| `embedding.rag.maxResults` | int | - | Max results to inject into context |
| `embedding.rag.collections` | []string | - | Collections to search (empty = all) |
| **Graph Store** | | | |
| `graph.enabled` | bool | `false` | Enable the knowledge graph store |
| `graph.backend` | string | `bolt` | Graph backend type (currently only `bolt`) |
| `graph.databasePath` | string | - | File path for graph database |
| `graph.maxTraversalDepth` | int | `2` | Maximum BFS traversal depth for graph expansion |
| `graph.maxExpansionResults` | int | `10` | Maximum graph-expanded results to return |
| **Multi-Agent** | | | |
| `agent.multiAgent` | bool | `false` | Enable hierarchical multi-agent orchestration |
| **A2A Protocol** | | | |
| `a2a.enabled` | bool | `false` | Enable A2A protocol support |
| `a2a.baseUrl` | string | - | External URL where this agent is reachable |
| `a2a.agentName` | string | - | Name advertised in the Agent Card |
| `a2a.agentDescription` | string | - | Description in the Agent Card |
| `a2a.remoteAgents` | []object | - | External A2A agents to integrate (name + agentCardUrl) |

## System Prompts

Lango ships with production-quality default prompts embedded in the binary. No configuration is needed â€” the agent works out of the box with prompts covering identity, safety, conversation rules, and tool usage guidelines.

### Prompt Sections

| File | Section | Priority | Description |
|------|---------|----------|-------------|
| `AGENTS.md` | Identity | 100 | Agent name, role, tool capabilities, knowledge system |
| `SAFETY.md` | Safety | 200 | Secret protection, destructive op confirmation, PII |
| `CONVERSATION_RULES.md` | Conversation Rules | 300 | Anti-repetition rules, channel limits, consistency |
| `TOOL_USAGE.md` | Tool Usage | 400 | Per-tool guidelines for exec, filesystem, browser, crypto, secrets |

### Customizing Prompts

Create a directory with `.md` files matching the section names above and set `agent.promptsDir`:

```bash
mkdir -p ~/.lango/prompts
# Override just the identity section
echo "You are a helpful coding assistant." > ~/.lango/prompts/AGENTS.md
```

Then configure the path via `lango onboard` > Agent Configuration > Prompts Directory, or set it in a config JSON:

```json
{
  "agent": {
    "promptsDir": "~/.lango/prompts"
  }
}
```

**Precedence:** `promptsDir` (directory) > `systemPromptPath` (legacy single file) > built-in defaults.

Unknown `.md` files in the directory are added as custom sections with priority 900+, appearing after the default sections.

## Embedding & RAG

Lango supports embedding-based retrieval-augmented generation (RAG) to inject relevant context into agent prompts automatically.

### Supported Providers

- **OpenAI** (`openai`): `text-embedding-3-small`, `text-embedding-3-large`, etc.
- **Google** (`google`): Gemini embedding models
- **Local** (`local`): Ollama-compatible local embedding server

### Configuration

Configure embedding and RAG settings via `lango onboard` > Embedding & RAG menu, or use `lango config` CLI.

### RAG

When `embedding.rag.enabled` is `true`, relevant knowledge entries are automatically retrieved via vector similarity search and injected into the agent's context. Configure `maxResults` to control how many results are included and `collections` to limit which knowledge collections are searched.

Use `lango doctor` to verify embedding configuration and provider connectivity.

## Knowledge Graph & Graph RAG

Lango includes a BoltDB-backed knowledge graph that stores relationships as Subject-Predicate-Object triples with three index orderings (SPO, POS, OSP) for efficient queries from any direction.

### Predicate Vocabulary

| Predicate | Meaning |
|-----------|---------|
| `related_to` | Semantic relationship between entities |
| `caused_by` | Causal relationship (effect â†’ cause) |
| `resolved_by` | Resolution relationship (error â†’ fix) |
| `follows` | Temporal ordering |
| `similar_to` | Similarity relationship |
| `contains` | Containment (session â†’ observation) |
| `in_session` | Session membership |
| `reflects_on` | Reflection targets |
| `learned_from` | Provenance (learning â†’ session) |

### Graph RAG (Hybrid Retrieval)

When both embedding/RAG and graph store are enabled, Lango uses 2-phase hybrid retrieval:

1. **Vector Search** â€” standard embedding-based similarity search
2. **Graph Expansion** â€” expands vector results through graph relationships (related_to, resolved_by, caused_by, similar_to)

This combines semantic similarity with structural knowledge for richer context.

### Self-Learning Graph

The `learning.GraphEngine` automatically records error patterns and fixes as graph triples, with confidence propagation (rate 0.3) that strengthens frequently-confirmed relationships.

### Configuration

Configure via `lango onboard` > Graph Store menu. Use `lango graph status`, `lango graph stats`, and `lango graph query` to inspect graph data.

## Multi-Agent Orchestration

When `agent.multiAgent` is enabled, Lango builds a hierarchical agent tree with specialized sub-agents:

| Agent | Role | Tools |
|-------|------|-------|
| **executor** | Runs tools: shell, filesystem, browser, crypto | exec, fs_*, browser_*, crypto_* |
| **researcher** | Knowledge retrieval, RAG, graph traversal | search_*, rag_*, graph_* |
| **planner** | Task decomposition and strategy | (reasoning only, no tools) |
| **memory-manager** | Memory operations and observations | memory_*, observe_*, reflect_* |

The orchestrator routes tasks to the appropriate sub-agent and synthesizes results. Tool partitioning is prefix-based â€” unmatched tools default to the executor.

Enable via `lango onboard` > Multi-Agent menu or set `agent.multiAgent: true` in import JSON. Use `lango agent status` and `lango agent list` to inspect.

## A2A Protocol

Lango supports the Agent-to-Agent (A2A) protocol for inter-agent communication:

- **Agent Card** â€” served at `/.well-known/agent.json` with agent name, description, skills
- **Remote Agents** â€” discover and integrate external A2A agents as sub-agents in the orchestrator
- **Graceful Degradation** â€” unreachable remote agents are skipped without blocking startup

Configure via `lango onboard` > A2A Protocol menu. Remote agents (name + URL pairs) should be configured via `lango config export` â†’ edit JSON â†’ `lango config import`.

> **Note:** All settings are stored in the encrypted profile database â€” no plaintext config files. Use `lango onboard` for interactive configuration or `lango config import/export` for programmatic configuration.

## Self-Learning System

Lango includes a self-learning knowledge system that improves agent performance over time.

- **Knowledge Store** - Persistent storage for facts, patterns, and external references
- **Learning Engine** - Observes tool execution results, extracts error patterns, boosts successful strategies
- **Skill System** - Agents can create reusable composite/script/template skills with safety validation
- **Context Retriever** - 8-layer context architecture that assembles relevant knowledge into prompts:
  1. Tool Registry â€” available tools and capabilities
  2. User Knowledge â€” rules, preferences, definitions, facts
  3. Skill Patterns â€” known working tool chains and workflows
  4. External Knowledge â€” docs, wiki, MCP integration
  5. Agent Learnings â€” error patterns, discovered fixes
  6. Runtime Context â€” session history, tool results, env state
  7. Observations â€” compressed conversation observations
  8. Reflections â€” condensed observation reflections

### Observational Memory

Observational Memory is an async subsystem that compresses long conversations into durable observations and reflections, keeping context relevant without exceeding token budgets.

- **Observer** â€” monitors conversation token count and produces compressed observations when the message token threshold is reached
- **Reflector** â€” condenses accumulated observations into higher-level reflections when the observation token threshold is reached
- **Async Buffer** â€” queues observation/reflection tasks for background processing
- **Token Counter** â€” tracks token usage to determine when compression should trigger

Configure knowledge and observational memory settings via `lango onboard` or `lango config` CLI. Use `lango memory list`, `lango memory status`, and `lango memory clear` to manage observation entries.

## Security

Lango includes built-in security features for AI agents:

### Security Configuration

Lango supports two security modes:

1. **Local Mode** (Default)
   - Encrypts secrets using AES-256-GCM derived from a passphrase (PBKDF2).
   - **Interactive**: Prompts for passphrase on startup (Recommended).
   - **Headless**: Set `LANGO_PASSPHRASE` environment variable.
   - **Migration**: Rotate your passphrase using:
     ```bash
     lango security migrate-passphrase
     ```
   > **âš ï¸ Warning**: Losing your passphrase results in permanent loss of all encrypted secrets. Lango does not store your passphrase.

2. **RPC Mode** (Production)
   - Offloads cryptographic operations to a hardware-backed companion app or external signer.
   - Keys never leave the secure hardware.

Configure security mode via `lango onboard` > Security menu, or use `lango config` CLI.

### AI Privacy Interceptor

Lango includes a privacy interceptor that sits between the agent and AI providers:

- **PII Redaction** â€” automatically detects and redacts personally identifiable information before sending to AI providers
- **Approval Workflows** â€” optionally require human approval before executing sensitive tools
- **Custom PII Patterns** â€” extend detection with custom regex patterns via `security.interceptor.piiRegexPatterns`

### Secret Management

Agents can manage encrypted secrets as part of their tool workflows. Secrets are stored using AES-256-GCM encryption and referenced by name, preventing plaintext values from appearing in logs or conversation history.

### Output Scanning

The built-in secret scanner monitors agent output for accidental secret leakage. Registered secret values are automatically replaced with `[SECRET:name]` placeholders before being displayed or logged.

### Key Registry

Lango manages cryptographic keys via an Ent-backed key registry. Keys are used for secret encryption, signing, and companion app integration.

### Companion App Discovery (RPC Mode)

Lango supports optional companion apps for hardware-backed security. Companion discovery is handled within the `internal/security` module:

- **mDNS Discovery** â€” auto-discovers companion apps on the local network via `_lango-companion._tcp`
- **Manual Config** â€” set a fixed companion address

### Authentication

Lango supports OIDC authentication for the gateway. Configure OIDC providers via `lango onboard` > Auth menu, or include them in a JSON config file and import with `lango config import`.

#### Auth Endpoints

| Method | Path | Description |
|--------|------|-------------|
| `GET` | `/auth/login/{provider}` | Initiate OIDC login flow |
| `GET` | `/auth/callback/{provider}` | OIDC callback (returns JSON: `{"status":"authenticated","sessionKey":"..."}`) |
| `POST` | `/auth/logout` | Clear session and cookie (returns JSON: `{"status":"logged_out"}`) |

#### Protected Routes

When OIDC is configured, the following endpoints require a valid `lango_session` cookie:
- `/ws` â€” WebSocket connection
- `/status` â€” Server status

Without OIDC configuration, all routes are open (development/local mode).

#### WebSocket CORS

Use `server.allowedOrigins` to control which origins can connect via WebSocket:
- `[]` (empty, default) â€” same-origin requests only
- `["https://example.com"]` â€” specific origins
- `["*"]` â€” allow all origins (not recommended for production)

#### Rate Limiting

Auth endpoints (`/auth/login/*`, `/auth/callback/*`, `/auth/logout`) are throttled to a maximum of 10 concurrent requests.

## Docker

```bash
# Build Docker image
make docker-build

# Run with docker-compose
docker-compose up -d
```

### Headless Configuration

The Docker image includes an entrypoint script that auto-imports configuration on first startup. Both the config and passphrase are injected via Docker secrets â€” never as environment variables â€” so the agent cannot read them at runtime.

1. Create `config.json` with your provider keys and settings.
2. Create `passphrase.txt` containing your encryption passphrase.
3. Run with docker-compose:
   ```bash
   docker-compose up -d
   ```

The entrypoint script (`docker-entrypoint.sh`):
- Copies the passphrase secret to `~/.lango/keyfile` (0600, blocked by the agent's filesystem tool)
- On first run, copies the config secret to `/tmp`, imports it into an encrypted profile, and the temp file is auto-deleted
- On subsequent restarts, the existing profile is reused

Environment variables (optional):
- `LANGO_PROFILE` â€” profile name to create (default: `default`)
- `LANGO_CONFIG_FILE` â€” override config secret path (default: `/run/secrets/lango_config`)
- `LANGO_PASSPHRASE_FILE` â€” override passphrase secret path (default: `/run/secrets/lango_passphrase`)

## Development

```bash
# Run tests with race detector
make test

# Run linter
make lint

# Build for all platforms
make build-all

# Run locally (build + serve)
make dev

# Generate Ent code
make generate

# Download and tidy dependencies
make deps
```

## License

MIT
